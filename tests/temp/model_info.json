{
  "framework": "vllm",
  "model_name": "facebook/opt-125m",
  "model_type": "vLLM",
  "parameters": "N/A",
  "input_info": {
    "type": "text_or_sampling_params",
    "sample_input": "{'prompts': ['Hello, my name is', 'The president of the United States is', 'The capital of France is', 'The future of AI is'], 'params': {'temperature': 0.0, 'max_tokens': 3}}"
  },
  "output_info": {
    "type": "generated_text",
    "sample_output": "[' J.C', ' not a racist', ' the capital of', ' in the hands']",
    "num_outputs": 4
  },
  "layers": [
    {
      "iter": 0,
      "ops_idx": 0,
      "module_name": "model.decoder.embed_tokens",
      "operator_name": "VocabParallelEmbedding",
      "input_shapes": [
        [
          26
        ]
      ],
      "input_dtypes": [
        "torch.int32"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 1,
      "module_name": "lm_head",
      "operator_name": "VocabParallelEmbedding",
      "input_shapes": [
        [
          26
        ]
      ],
      "input_dtypes": [
        "torch.int32"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 2,
      "module_name": "model.decoder.embed_positions",
      "operator_name": "OPTLearnedPositionalEmbedding",
      "input_shapes": [
        [
          26
        ]
      ],
      "input_dtypes": [
        "torch.int64"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 3,
      "module_name": "model.decoder.layers.0.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 4,
      "module_name": "model.decoder.layers.0.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 5,
      "module_name": "model.decoder.layers.0.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 6,
      "module_name": "model.decoder.layers.0.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 7,
      "module_name": "model.decoder.layers.0.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 8,
      "module_name": "model.decoder.layers.0.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 9,
      "module_name": "model.decoder.layers.0.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 10,
      "module_name": "model.decoder.layers.0.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 11,
      "module_name": "model.decoder.layers.1.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 12,
      "module_name": "model.decoder.layers.1.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 13,
      "module_name": "model.decoder.layers.1.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 14,
      "module_name": "model.decoder.layers.1.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 15,
      "module_name": "model.decoder.layers.1.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 16,
      "module_name": "model.decoder.layers.1.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 17,
      "module_name": "model.decoder.layers.1.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 18,
      "module_name": "model.decoder.layers.1.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 19,
      "module_name": "model.decoder.layers.2.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 20,
      "module_name": "model.decoder.layers.2.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 21,
      "module_name": "model.decoder.layers.2.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 22,
      "module_name": "model.decoder.layers.2.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 23,
      "module_name": "model.decoder.layers.2.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 24,
      "module_name": "model.decoder.layers.2.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 25,
      "module_name": "model.decoder.layers.2.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 26,
      "module_name": "model.decoder.layers.2.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 27,
      "module_name": "model.decoder.layers.3.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 28,
      "module_name": "model.decoder.layers.3.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 29,
      "module_name": "model.decoder.layers.3.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 30,
      "module_name": "model.decoder.layers.3.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 31,
      "module_name": "model.decoder.layers.3.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 32,
      "module_name": "model.decoder.layers.3.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 33,
      "module_name": "model.decoder.layers.3.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 34,
      "module_name": "model.decoder.layers.3.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 35,
      "module_name": "model.decoder.layers.4.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 36,
      "module_name": "model.decoder.layers.4.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 37,
      "module_name": "model.decoder.layers.4.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 38,
      "module_name": "model.decoder.layers.4.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 39,
      "module_name": "model.decoder.layers.4.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 40,
      "module_name": "model.decoder.layers.4.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 41,
      "module_name": "model.decoder.layers.4.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 42,
      "module_name": "model.decoder.layers.4.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 43,
      "module_name": "model.decoder.layers.5.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 44,
      "module_name": "model.decoder.layers.5.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 45,
      "module_name": "model.decoder.layers.5.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 46,
      "module_name": "model.decoder.layers.5.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 47,
      "module_name": "model.decoder.layers.5.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 48,
      "module_name": "model.decoder.layers.5.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 49,
      "module_name": "model.decoder.layers.5.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 50,
      "module_name": "model.decoder.layers.5.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 51,
      "module_name": "model.decoder.layers.6.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 52,
      "module_name": "model.decoder.layers.6.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 53,
      "module_name": "model.decoder.layers.6.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 54,
      "module_name": "model.decoder.layers.6.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 55,
      "module_name": "model.decoder.layers.6.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 56,
      "module_name": "model.decoder.layers.6.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 57,
      "module_name": "model.decoder.layers.6.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 58,
      "module_name": "model.decoder.layers.6.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 59,
      "module_name": "model.decoder.layers.7.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 60,
      "module_name": "model.decoder.layers.7.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 61,
      "module_name": "model.decoder.layers.7.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 62,
      "module_name": "model.decoder.layers.7.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 63,
      "module_name": "model.decoder.layers.7.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 64,
      "module_name": "model.decoder.layers.7.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 65,
      "module_name": "model.decoder.layers.7.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 66,
      "module_name": "model.decoder.layers.7.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 67,
      "module_name": "model.decoder.layers.8.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 68,
      "module_name": "model.decoder.layers.8.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 69,
      "module_name": "model.decoder.layers.8.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 70,
      "module_name": "model.decoder.layers.8.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 71,
      "module_name": "model.decoder.layers.8.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 72,
      "module_name": "model.decoder.layers.8.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 73,
      "module_name": "model.decoder.layers.8.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 74,
      "module_name": "model.decoder.layers.8.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 75,
      "module_name": "model.decoder.layers.9.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 76,
      "module_name": "model.decoder.layers.9.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 77,
      "module_name": "model.decoder.layers.9.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 78,
      "module_name": "model.decoder.layers.9.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 79,
      "module_name": "model.decoder.layers.9.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 80,
      "module_name": "model.decoder.layers.9.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 81,
      "module_name": "model.decoder.layers.9.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 82,
      "module_name": "model.decoder.layers.9.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 83,
      "module_name": "model.decoder.layers.10.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 84,
      "module_name": "model.decoder.layers.10.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 85,
      "module_name": "model.decoder.layers.10.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 86,
      "module_name": "model.decoder.layers.10.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 87,
      "module_name": "model.decoder.layers.10.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 88,
      "module_name": "model.decoder.layers.10.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 89,
      "module_name": "model.decoder.layers.10.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 90,
      "module_name": "model.decoder.layers.10.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 91,
      "module_name": "model.decoder.layers.11.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 92,
      "module_name": "model.decoder.layers.11.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 93,
      "module_name": "model.decoder.layers.11.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          26,
          768
        ],
        [
          26,
          768
        ],
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 94,
      "module_name": "model.decoder.layers.11.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 95,
      "module_name": "model.decoder.layers.11.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 96,
      "module_name": "model.decoder.layers.11.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 97,
      "module_name": "model.decoder.layers.11.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 98,
      "module_name": "model.decoder.layers.11.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          26,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 99,
      "module_name": "model.decoder.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          26,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          26,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 0,
      "ops_idx": 100,
      "module_name": "logits_processor",
      "operator_name": "LogitsProcessor",
      "input_shapes": [
        [],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "VocabParallelEmbedding",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          50272
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 0,
      "module_name": "model.decoder.embed_tokens",
      "operator_name": "VocabParallelEmbedding",
      "input_shapes": [
        [
          4
        ]
      ],
      "input_dtypes": [
        "torch.int32"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 1,
      "module_name": "lm_head",
      "operator_name": "VocabParallelEmbedding",
      "input_shapes": [
        [
          4
        ]
      ],
      "input_dtypes": [
        "torch.int32"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 2,
      "module_name": "model.decoder.embed_positions",
      "operator_name": "OPTLearnedPositionalEmbedding",
      "input_shapes": [
        [
          4
        ]
      ],
      "input_dtypes": [
        "torch.int64"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 3,
      "module_name": "model.decoder.layers.0.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 4,
      "module_name": "model.decoder.layers.0.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 5,
      "module_name": "model.decoder.layers.0.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 6,
      "module_name": "model.decoder.layers.0.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 7,
      "module_name": "model.decoder.layers.0.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 8,
      "module_name": "model.decoder.layers.0.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 9,
      "module_name": "model.decoder.layers.0.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 10,
      "module_name": "model.decoder.layers.0.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 11,
      "module_name": "model.decoder.layers.1.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 12,
      "module_name": "model.decoder.layers.1.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 13,
      "module_name": "model.decoder.layers.1.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 14,
      "module_name": "model.decoder.layers.1.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 15,
      "module_name": "model.decoder.layers.1.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 16,
      "module_name": "model.decoder.layers.1.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 17,
      "module_name": "model.decoder.layers.1.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 18,
      "module_name": "model.decoder.layers.1.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 19,
      "module_name": "model.decoder.layers.2.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 20,
      "module_name": "model.decoder.layers.2.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 21,
      "module_name": "model.decoder.layers.2.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 22,
      "module_name": "model.decoder.layers.2.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 23,
      "module_name": "model.decoder.layers.2.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 24,
      "module_name": "model.decoder.layers.2.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 25,
      "module_name": "model.decoder.layers.2.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 26,
      "module_name": "model.decoder.layers.2.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 27,
      "module_name": "model.decoder.layers.3.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 28,
      "module_name": "model.decoder.layers.3.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 29,
      "module_name": "model.decoder.layers.3.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 30,
      "module_name": "model.decoder.layers.3.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 31,
      "module_name": "model.decoder.layers.3.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 32,
      "module_name": "model.decoder.layers.3.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 33,
      "module_name": "model.decoder.layers.3.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 34,
      "module_name": "model.decoder.layers.3.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 35,
      "module_name": "model.decoder.layers.4.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 36,
      "module_name": "model.decoder.layers.4.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 37,
      "module_name": "model.decoder.layers.4.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 38,
      "module_name": "model.decoder.layers.4.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 39,
      "module_name": "model.decoder.layers.4.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 40,
      "module_name": "model.decoder.layers.4.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 41,
      "module_name": "model.decoder.layers.4.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 42,
      "module_name": "model.decoder.layers.4.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 43,
      "module_name": "model.decoder.layers.5.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 44,
      "module_name": "model.decoder.layers.5.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 45,
      "module_name": "model.decoder.layers.5.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 46,
      "module_name": "model.decoder.layers.5.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 47,
      "module_name": "model.decoder.layers.5.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 48,
      "module_name": "model.decoder.layers.5.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 49,
      "module_name": "model.decoder.layers.5.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 50,
      "module_name": "model.decoder.layers.5.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 51,
      "module_name": "model.decoder.layers.6.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 52,
      "module_name": "model.decoder.layers.6.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 53,
      "module_name": "model.decoder.layers.6.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 54,
      "module_name": "model.decoder.layers.6.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 55,
      "module_name": "model.decoder.layers.6.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 56,
      "module_name": "model.decoder.layers.6.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 57,
      "module_name": "model.decoder.layers.6.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 58,
      "module_name": "model.decoder.layers.6.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 59,
      "module_name": "model.decoder.layers.7.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 60,
      "module_name": "model.decoder.layers.7.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 61,
      "module_name": "model.decoder.layers.7.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 62,
      "module_name": "model.decoder.layers.7.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 63,
      "module_name": "model.decoder.layers.7.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 64,
      "module_name": "model.decoder.layers.7.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 65,
      "module_name": "model.decoder.layers.7.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 66,
      "module_name": "model.decoder.layers.7.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 67,
      "module_name": "model.decoder.layers.8.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 68,
      "module_name": "model.decoder.layers.8.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 69,
      "module_name": "model.decoder.layers.8.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 70,
      "module_name": "model.decoder.layers.8.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 71,
      "module_name": "model.decoder.layers.8.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 72,
      "module_name": "model.decoder.layers.8.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 73,
      "module_name": "model.decoder.layers.8.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 74,
      "module_name": "model.decoder.layers.8.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 75,
      "module_name": "model.decoder.layers.9.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 76,
      "module_name": "model.decoder.layers.9.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 77,
      "module_name": "model.decoder.layers.9.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 78,
      "module_name": "model.decoder.layers.9.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 79,
      "module_name": "model.decoder.layers.9.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 80,
      "module_name": "model.decoder.layers.9.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 81,
      "module_name": "model.decoder.layers.9.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 82,
      "module_name": "model.decoder.layers.9.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 83,
      "module_name": "model.decoder.layers.10.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 84,
      "module_name": "model.decoder.layers.10.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 85,
      "module_name": "model.decoder.layers.10.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 86,
      "module_name": "model.decoder.layers.10.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 87,
      "module_name": "model.decoder.layers.10.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 88,
      "module_name": "model.decoder.layers.10.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 89,
      "module_name": "model.decoder.layers.10.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 90,
      "module_name": "model.decoder.layers.10.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 91,
      "module_name": "model.decoder.layers.11.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 92,
      "module_name": "model.decoder.layers.11.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 93,
      "module_name": "model.decoder.layers.11.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 94,
      "module_name": "model.decoder.layers.11.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 95,
      "module_name": "model.decoder.layers.11.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 96,
      "module_name": "model.decoder.layers.11.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 97,
      "module_name": "model.decoder.layers.11.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 98,
      "module_name": "model.decoder.layers.11.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 99,
      "module_name": "model.decoder.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 1,
      "ops_idx": 100,
      "module_name": "logits_processor",
      "operator_name": "LogitsProcessor",
      "input_shapes": [
        [],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "VocabParallelEmbedding",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          50272
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 0,
      "module_name": "model.decoder.embed_tokens",
      "operator_name": "VocabParallelEmbedding",
      "input_shapes": [
        [
          4
        ]
      ],
      "input_dtypes": [
        "torch.int32"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 1,
      "module_name": "lm_head",
      "operator_name": "VocabParallelEmbedding",
      "input_shapes": [
        [
          4
        ]
      ],
      "input_dtypes": [
        "torch.int32"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 2,
      "module_name": "model.decoder.embed_positions",
      "operator_name": "OPTLearnedPositionalEmbedding",
      "input_shapes": [
        [
          4
        ]
      ],
      "input_dtypes": [
        "torch.int64"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 3,
      "module_name": "model.decoder.layers.0.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 4,
      "module_name": "model.decoder.layers.0.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 5,
      "module_name": "model.decoder.layers.0.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 6,
      "module_name": "model.decoder.layers.0.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 7,
      "module_name": "model.decoder.layers.0.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 8,
      "module_name": "model.decoder.layers.0.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 9,
      "module_name": "model.decoder.layers.0.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 10,
      "module_name": "model.decoder.layers.0.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 11,
      "module_name": "model.decoder.layers.1.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 12,
      "module_name": "model.decoder.layers.1.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 13,
      "module_name": "model.decoder.layers.1.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 14,
      "module_name": "model.decoder.layers.1.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 15,
      "module_name": "model.decoder.layers.1.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 16,
      "module_name": "model.decoder.layers.1.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 17,
      "module_name": "model.decoder.layers.1.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 18,
      "module_name": "model.decoder.layers.1.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 19,
      "module_name": "model.decoder.layers.2.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 20,
      "module_name": "model.decoder.layers.2.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 21,
      "module_name": "model.decoder.layers.2.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 22,
      "module_name": "model.decoder.layers.2.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 23,
      "module_name": "model.decoder.layers.2.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 24,
      "module_name": "model.decoder.layers.2.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 25,
      "module_name": "model.decoder.layers.2.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 26,
      "module_name": "model.decoder.layers.2.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 27,
      "module_name": "model.decoder.layers.3.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 28,
      "module_name": "model.decoder.layers.3.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 29,
      "module_name": "model.decoder.layers.3.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 30,
      "module_name": "model.decoder.layers.3.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 31,
      "module_name": "model.decoder.layers.3.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 32,
      "module_name": "model.decoder.layers.3.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 33,
      "module_name": "model.decoder.layers.3.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 34,
      "module_name": "model.decoder.layers.3.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 35,
      "module_name": "model.decoder.layers.4.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 36,
      "module_name": "model.decoder.layers.4.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 37,
      "module_name": "model.decoder.layers.4.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 38,
      "module_name": "model.decoder.layers.4.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 39,
      "module_name": "model.decoder.layers.4.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 40,
      "module_name": "model.decoder.layers.4.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 41,
      "module_name": "model.decoder.layers.4.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 42,
      "module_name": "model.decoder.layers.4.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 43,
      "module_name": "model.decoder.layers.5.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 44,
      "module_name": "model.decoder.layers.5.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 45,
      "module_name": "model.decoder.layers.5.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 46,
      "module_name": "model.decoder.layers.5.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 47,
      "module_name": "model.decoder.layers.5.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 48,
      "module_name": "model.decoder.layers.5.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 49,
      "module_name": "model.decoder.layers.5.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 50,
      "module_name": "model.decoder.layers.5.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 51,
      "module_name": "model.decoder.layers.6.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 52,
      "module_name": "model.decoder.layers.6.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 53,
      "module_name": "model.decoder.layers.6.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 54,
      "module_name": "model.decoder.layers.6.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 55,
      "module_name": "model.decoder.layers.6.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 56,
      "module_name": "model.decoder.layers.6.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 57,
      "module_name": "model.decoder.layers.6.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 58,
      "module_name": "model.decoder.layers.6.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 59,
      "module_name": "model.decoder.layers.7.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 60,
      "module_name": "model.decoder.layers.7.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 61,
      "module_name": "model.decoder.layers.7.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 62,
      "module_name": "model.decoder.layers.7.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 63,
      "module_name": "model.decoder.layers.7.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 64,
      "module_name": "model.decoder.layers.7.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 65,
      "module_name": "model.decoder.layers.7.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 66,
      "module_name": "model.decoder.layers.7.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 67,
      "module_name": "model.decoder.layers.8.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 68,
      "module_name": "model.decoder.layers.8.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 69,
      "module_name": "model.decoder.layers.8.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 70,
      "module_name": "model.decoder.layers.8.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 71,
      "module_name": "model.decoder.layers.8.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 72,
      "module_name": "model.decoder.layers.8.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 73,
      "module_name": "model.decoder.layers.8.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 74,
      "module_name": "model.decoder.layers.8.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 75,
      "module_name": "model.decoder.layers.9.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 76,
      "module_name": "model.decoder.layers.9.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 77,
      "module_name": "model.decoder.layers.9.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 78,
      "module_name": "model.decoder.layers.9.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 79,
      "module_name": "model.decoder.layers.9.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 80,
      "module_name": "model.decoder.layers.9.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 81,
      "module_name": "model.decoder.layers.9.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 82,
      "module_name": "model.decoder.layers.9.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 83,
      "module_name": "model.decoder.layers.10.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 84,
      "module_name": "model.decoder.layers.10.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 85,
      "module_name": "model.decoder.layers.10.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 86,
      "module_name": "model.decoder.layers.10.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 87,
      "module_name": "model.decoder.layers.10.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 88,
      "module_name": "model.decoder.layers.10.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 89,
      "module_name": "model.decoder.layers.10.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 90,
      "module_name": "model.decoder.layers.10.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 91,
      "module_name": "model.decoder.layers.11.self_attn_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 92,
      "module_name": "model.decoder.layers.11.self_attn.qkv_proj",
      "operator_name": "QKVParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          2304
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 93,
      "module_name": "model.decoder.layers.11.self_attn.attn",
      "operator_name": "Attention",
      "input_shapes": [
        [
          4,
          768
        ],
        [
          4,
          768
        ],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16",
        "torch.float16",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 94,
      "module_name": "model.decoder.layers.11.self_attn.out_proj",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 95,
      "module_name": "model.decoder.layers.11.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 96,
      "module_name": "model.decoder.layers.11.fc1",
      "operator_name": "ColumnParallelLinear",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 97,
      "module_name": "model.decoder.layers.11.activation_fn",
      "operator_name": "ReLU",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          3072
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 98,
      "module_name": "model.decoder.layers.11.fc2",
      "operator_name": "RowParallelLinear",
      "input_shapes": [
        [
          4,
          3072
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ],
        []
      ],
      "output_dtypes": [
        "float16",
        "object"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 99,
      "module_name": "model.decoder.final_layer_norm",
      "operator_name": "LayerNorm",
      "input_shapes": [
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          768
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    },
    {
      "iter": 2,
      "ops_idx": 100,
      "module_name": "logits_processor",
      "operator_name": "LogitsProcessor",
      "input_shapes": [
        [],
        [
          4,
          768
        ]
      ],
      "input_dtypes": [
        "VocabParallelEmbedding",
        "torch.float16"
      ],
      "output_shapes": [
        [
          4,
          50272
        ]
      ],
      "output_dtypes": [
        "torch.float16"
      ]
    }
  ]
}